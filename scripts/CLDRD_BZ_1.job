#!/bin/sh
#SBATCH -o ./logs/%A_%a.out
#SBATCH -e ./logs/%A_%a.err
#SBATCH -n1
#SBATCH --partition=gpu
#SBATCH -c12
#SBATCH --mem=120G
#SBATCH --time=99:00:00
#SBATCH --array=1-4%4
#SBATCH --gres=gpu:1

# Set-up the environment. double curly brace necessary: https://stackoverflow.com/a/5466478
source ${HOME}\/.bashrc
conda activate multivariate_ir

# load param file
HPARAMS_FILE=CLDRD_BZ_1.params

# echo run info
echo "SLURM_SUBMIT_DIR="$SLURM_SUBMIT_DIR
echo "SLURM_JOB_ID"=$SLURM_JOB_ID
echo "SLURM_JOB_NAME"=$SLURM_JOB_NAME
echo $SLURM_SUBMIT_DIR

# Start the experiment.
CUDA_VISIBLE_DEVICES=0 python -m tevatron.driver.train_DRD \
  --model_name_or_path sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco \
  --teacher_model_name_or_path cross-encoder/ms-marco-MiniLM-L-6-v2 \
  --evaluation_strategy steps \
  --do_train \
  --do_eval \
  --exclude_title \
  --add_var_token \
  --embed_formulation updated \
  --kd_type cldrd \
  --pseudolabels \
  --ann_neg_num 30 \
  --group_1 5 \
  --group_2 12 \
  --group_3 13 \
  --group_1_size 5 \
  --group_2_size 45 \
  --group_3_size 150 \
  --train_n_passages 1 \
  --grad_cache \
  --gc_q_chunk_size 15 \
  --gc_p_chunk_size 15 \
  --dataset_name Tevatron/msmarco-passage \
  --train_dir /scratch-shared/sbhargav/data/train_reranked_MiniLM_200_cmp_no_title \
  --val_dir /scratch-shared/sbhargav/data/validation \
  --fp16 \
  --fp16_full_eval \
  --learning_rate 5e-6 \
  --q_max_len 32 \
  --p_max_len 256 \
  --warmup_ratio 0.1 \
  --logging_steps 15 \
  --evaluation_strategy steps \
  --cache_dir /scratch-shared/sbhargav/data/cache_models \
  --data_cache_dir /scratch-shared/sbhargav/data/train_reranked_MiniLM_200_cmp_no_title \
  --disable_distributed $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)

