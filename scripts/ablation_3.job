#!/bin/sh
# The following lines instruct Slurm to allocate one GPU.
#SBATCH -o ./%A.out
#SBATCH -e ./%A.err
#SBATCH --partition=gpu
#SBATCH --gres=gpu:nvidia_rtx_a6000:1
#SBATCH -c12
#SBATCH --mem=28G
#SBATCH --time=96:00:00

source ${HOME}\/.bashrc
conda activate multivariate_ir

# echo run info
echo "SLURM_SUBMIT_DIR="$SLURM_SUBMIT_DIR
echo "SLURM_JOB_ID"=$SLURM_JOB_ID
echo "SLURM_JOB_NAME"=$SLURM_JOB_NAME


CUDA_VISIBLE_DEVICES=0 python -m tevatron.driver.train_DRD \
  --output_dir /ivi/ilps/projects/multivariate_ir/experiments_gs/Snellius/gradcache_exp/ablations/train_MVRL_BM25_ANN_inbatch_raw_15_b_25_lr_506_25_6_LOGVAR \
  --model_name_or_path sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco \
  --teacher_model_name_or_path cross-encoder/ms-marco-MiniLM-L-6-v2 \
  --do_train \
  --do_eval \
  --exclude_title \
  --model_type mvrl \
  --var_activation logvar \
  --add_var_token \
  --embed_formulation updated \
  --kd_type drd \
  --kd_in_batch_negs \
  --ann_neg_num 25 \
  --train_n_passages 6 \
  --per_device_train_batch_size 15 \
  --dataset_name Tevatron/msmarco-passage \
  --train_dir /ivi/ilps/projects/multivariate_ir/experiments_gs/data/train_tasb_ann_negs_50/combined \
  --val_dir /ivi/ilps/projects/multivariate_ir/experiments_gs/data/validation \
  --fp16 \
  --fp16_full_eval \
  --learning_rate 5e-6 \
  --q_max_len 32 \
  --p_max_len 256 \
  --warmup_ratio 0.1 \
  --max_steps 200000 \
  --logging_steps 150 \
  --evaluation_strategy steps \
  --eval_steps 25000 \
  --save_steps 25000 \
  --cache_dir /ivi/ilps/projects/multivariate_ir/experiments_gs/MVRL/cache/cache_models \
  --data_cache_dir /ivi/ilps/projects/multivariate_ir/experiments_gs/data/train_reranked_MiniLM_200_cmp_no_title \
  --disable_distributed \
  --overwrite_output_dir
