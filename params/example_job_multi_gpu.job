#!/bin/sh
#SBATCH -o /ivi/ilps/projects/multivariate_ir/logs/%A_%a.out
#SBATCH -e /ivi/ilps/projects/multivariate_ir/logs/%A_%a.err
#SBATCH -n1
#SBATCH --partition=gpu
#SBATCH -c12
#SBATCH --mem=40G
#SBATCH --time=24:00:00
#SBATCH --array=1-20%8
#SBATCH --gres=gpu:2
#SBATCH --exclude=ilps-cn111,ilps-cn108

# Set-up the environment. double curly brace necessary: https://stackoverflow.com/a/5466478
source ${HOME}\/.bashrc

# load param file
HPARAMS_FILE=example_job.params

# echo run info
echo "SLURM_SUBMIT_DIR="$SLURM_SUBMIT_DIR
echo "SLURM_JOB_ID"=$SLURM_JOB_ID
echo "SLURM_JOB_NAME"=$SLURM_JOB_NAME
echo $SLURM_SUBMIT_DIR

# Start the experiment.
LOCAL_RANK=0,1 CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 --use-env -m tevatron.driver.train $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)
